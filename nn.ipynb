{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Base object for fitting to a sequence of data, such as a dataset.\n",
       "\n",
       "Every `Sequence` must implement the `__getitem__` and the `__len__` methods.\n",
       "If you want to modify your dataset between epochs you may implement\n",
       "`on_epoch_end`.\n",
       "The method `__getitem__` should return a complete batch.\n",
       "\n",
       "Notes:\n",
       "\n",
       "`Sequence` are a safer way to do multiprocessing. This structure guarantees\n",
       "that the network will only train once\n",
       " on each sample per epoch which is not the case with generators.\n",
       "\n",
       "Examples:\n",
       "\n",
       "```python\n",
       "    from skimage.io import imread\n",
       "    from skimage.transform import resize\n",
       "    import numpy as np\n",
       "    import math\n",
       "\n",
       "    # Here, `x_set` is list of path to the images\n",
       "    # and `y_set` are the associated classes.\n",
       "\n",
       "    class CIFAR10Sequence(Sequence):\n",
       "\n",
       "        def __init__(self, x_set, y_set, batch_size):\n",
       "            self.x, self.y = x_set, y_set\n",
       "            self.batch_size = batch_size\n",
       "\n",
       "        def __len__(self):\n",
       "            return math.ceil(len(self.x) / self.batch_size)\n",
       "\n",
       "        def __getitem__(self, idx):\n",
       "            batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
       "            self.batch_size]\n",
       "            batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
       "            self.batch_size]\n",
       "\n",
       "            return np.array([\n",
       "                resize(imread(file_name), (200, 200))\n",
       "                   for file_name in batch_x]), np.array(batch_y)\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/envs/tensorflow2_py3/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/data_utils.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Iterator, Iterator, TimeseriesGenerator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_py3",
   "language": "python",
   "name": "conda_tensorflow2_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
